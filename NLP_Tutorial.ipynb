{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObudBS1Tsth/N5QmA/oUnO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weibb123/NLP_tutorial/blob/main/NLP_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SER_RbJjP58Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import svm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "C8m_Z8rgV5bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words\n",
        "Turn arbitrary text into fixed-length vectors by counting how many times each word appears."
      ],
      "metadata": {
        "id": "aAx6S2SeP9lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a text classification using Bag-of-words(SVM)\n",
        "class Category:\n",
        "  BOOKS = \"BOOKS\"\n",
        "  CLOTHING = \"CLOTHING\"\n",
        "\n",
        "train_x = [\"i love the book\", 'this is a great book', 'the fit is great', 'i love the shoes']\n",
        "train_y = [Category.BOOKS, Category.BOOKS, Category.CLOTHING, Category.CLOTHING]"
      ],
      "metadata": {
        "id": "ScxCImINSMNY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "train_x_vectors = vectorizer.fit_transform(train_x)\n",
        "\n",
        "print(vectorizer.get_feature_names())\n",
        "print(train_x_vectors.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_syFgEEQ_pq",
        "outputId": "3816f267-3ddf-4b04-c484-33e065bbb0bc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['book', 'fit', 'great', 'is', 'love', 'shoes', 'the', 'this']\n",
            "[[1 0 0 0 1 0 1 0]\n",
            " [1 0 1 1 0 0 0 1]\n",
            " [0 1 1 1 0 0 1 0]\n",
            " [0 0 0 0 1 1 1 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_svm = svm.SVC(kernel='linear')\n",
        "clf_svm.fit(train_x_vectors, train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ3Y4vQjQqwQ",
        "outputId": "602ab50e-f7b7-4a30-bcc9-f73298600ca6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_x = vectorizer.transform(['i like the book'])\n",
        "\n",
        "clf_svm.predict(test_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWVRh6bhTHFT",
        "outputId": "49d3a116-eafc-4c65-81e5-363232b10335"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['BOOKS'], dtype='<U8')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Issue with this model\n",
        "test_x = vectorizer.transform(['i like the story'])\n",
        "\n",
        "clf_svm.predict(test_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RazjRMFRTNSO",
        "outputId": "23531748-3aac-482c-8026-00ff83c92b2b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['CLOTHING'], dtype='<U8')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Bag of Words is great on the stuff that is trained on., but if it doesn't seen a word, it fails miserably</b>"
      ],
      "metadata": {
        "id": "mOfSlPLJULzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Vectors\n",
        "\n",
        "Convert text into numerical vector and map onto Vector Space\n",
        "\n",
        "Words that are associated to each other are closer together in Vector Space\\\n",
        "Ex. I like apple, I like fruits are closely together\n",
        "\n",
        "**Capture semantic**"
      ],
      "metadata": {
        "id": "v8Z2kHjlUZXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_md') # Word vector space model from spacy"
      ],
      "metadata": {
        "id": "OlKMgCpoUPFq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZSSGmElXgmW",
        "outputId": "7564e930-ae03-412c-f6eb-6772d90b37e1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i love the book', 'this is a great book', 'the fit is great', 'i love the shoes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [nlp(text) for text in train_x]\n",
        "train_x_word_vectors = [x.vector for x in docs]"
      ],
      "metadata": {
        "id": "TnGKMqraW0Yi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_svm_wv = svm.SVC(kernel='linear')\n",
        "clf_svm_wv.fit(train_x_word_vectors, train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VSwiGXhXlkO",
        "outputId": "d40b27a7-a394-4ebb-b4fd-12b2ce83bb50"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict using word vector model\n",
        "test_x = [\"i love the story\"]\n",
        "test_docs = [nlp(text) for text in test_x]\n",
        "test_x_word_vectors = [x.vector for x in test_docs]\n",
        "\n",
        "clf_svm_wv.predict(test_x_word_vectors)\n",
        "\n",
        "# This solves the issue that Bag-Word cannot handle !"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDZUVcFLYaAk",
        "outputId": "f84a6f00-4d64-48b3-e361-a99a217f32de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['BOOKS'], dtype='<U8')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disadvantage of Word Vector not work well with long text and multi-class**\n",
        "\n",
        "**Not do well when some words have different meaning in sentence**"
      ],
      "metadata": {
        "id": "8ImQ2UEEZ1ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regexes\n",
        "\n",
        "Pattern Matching of strings in Python\n",
        "\n",
        "Password checkers, phone numbers, emails, and more\n",
        "\n",
        "123-123-1234 555-555-5555 + 1-(123)-123-1234"
      ],
      "metadata": {
        "id": "t7gz8dBqaWnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "regexp = re.compile(r\"\\bread\\b|\\bstory\\b|book\") #\\b gets the word in between. so we can story not history\n",
        "\n",
        "phrases = ['I liked that history', 'i like that book', 'this hat is nice', 'the car tread up the hill']\n",
        "\n",
        "matches = []\n",
        "for i in phrases:\n",
        "  if re.search(regexp, i):\n",
        "    matches.append(i)\n",
        "\n",
        "print(matches)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV5tlUj5af3K",
        "outputId": "6f5f4ecd-9527-46c1-ef6b-8022368f4136"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i like that book']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming/Lemmatization\n",
        "\n",
        "Use it to normalize text\n",
        "\n",
        "reading -> read\\\n",
        "Books -> book\\\n",
        "Stories -> stori for stemming, story for lemmatizing"
      ],
      "metadata": {
        "id": "MSAX-mPAlW02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8EQk9BGleon",
        "outputId": "f391c8de-b0dc-4319-9f3e-5cbb93ff1703"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "phrase = 'reading the books'\n",
        "words = word_tokenize(phrase)\n",
        "\n",
        "stemmed_words = []\n",
        "for i in words:\n",
        "  stemmed_words.append(stemmer.stem(i))\n",
        "\n",
        "\" \".join(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cVhzfyBel-jX",
        "outputId": "7097801b-1739-4898-9cc9-1d089b7f033e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'read the book'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "you see how \"reading the books\" -> \"read the book\""
      ],
      "metadata": {
        "id": "KyqZepzHmgtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying Lemmatize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "phrase = 'reading the books'\n",
        "words = word_tokenize(phrase)\n",
        "\n",
        "lemmatized_words = []\n",
        "for i in words:\n",
        "  lemmatized_words.append(lemmatizer.lemmatize(i))\n",
        "\n",
        "\" \".join(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "isDqeLAAmIcy",
        "outputId": "cdffd0ef-6214-4aa6-8dc6-cf2439706fa9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'reading the book'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stopword Removal\n",
        "(Remove most common words from sentences)\n",
        "\n",
        "This might help us in word-vector model, more capture of meaning "
      ],
      "metadata": {
        "id": "zxO2zHv8ooE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english') # common words in english\n",
        "\n",
        "phrase = 'Here is an example sentence demonstrating the removal of stopwords'\n",
        "\n",
        "words = word_tokenize(phrase)\n",
        "\n",
        "stripped_phrase = []\n",
        "for i in words:\n",
        "  if i not in stop_words:\n",
        "    stripped_phrase.append(i)\n",
        "\n",
        "\" \".join(stripped_phrase)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0Lq9j7CcoImC",
        "outputId": "60c46e4a-3686-4bb7-f119-46e0e6b29862"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here example sentence demonstrating removal stopwords'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Various other techniques (spell correction, sentiment, & part speech tagging)"
      ],
      "metadata": {
        "id": "44SbaHdwpsre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "id": "VwZN9Pgyqac6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "phrase = 'i hate reading this book'\n",
        "\n",
        "tb_phrase = TextBlob(phrase)\n",
        "print(tb_phrase.correct())\n",
        "\n",
        "print(tb_phrase.tags)\n",
        "\n",
        "print(tb_phrase.sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pckCrdNko40K",
        "outputId": "0c364929-ced5-45c9-a938-916967e6a22c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i hate reading this book\n",
            "[('i', 'JJ'), ('hate', 'VBP'), ('reading', 'VBG'), ('this', 'DT'), ('book', 'NN')]\n",
            "Sentiment(polarity=-0.8, subjectivity=0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DT -> Determiner\\\n",
        "NN -> singular noun\\\n",
        "VBD -> verb\n"
      ],
      "metadata": {
        "id": "nNQh2vHAqxN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer\n",
        "\n",
        "Feed in a phrase, As you iterate through the token in the phrase, you can basically figure out what each token needs extra attention."
      ],
      "metadata": {
        "id": "5EpbXeWWrfAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spascy-transformers\n",
        "!python -m spacy download en_trf_bertbaseuncased_lg"
      ],
      "metadata": {
        "id": "xo9WrP02riPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import torch\n",
        "\n",
        "nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
        "doc = nlp(\"Here is some text to encode.\")"
      ],
      "metadata": {
        "id": "hVP5kBv6uJcx"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}